# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FDcGuafk8rt6O7JGqBNo7-WMI188R2Wi
"""

import streamlit as st
import joblib
import re
import nltk
import ssl

# Fix SSL certificate issues for NLTK downloads
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

# --- NLTK Data Download ---
@st.cache_data
def download_nltk_data():
    """Download required NLTK data and return stopwords set."""
    try:
        # Download required NLTK data
        nltk.download('punkt', quiet=True)
        nltk.download('punkt_tab', quiet=True)  # For newer NLTK versions
        nltk.download('stopwords', quiet=True)
        
        # Import after download
        from nltk.corpus import stopwords
        return set(stopwords.words('english'))
    except Exception as e:
        st.error(f"Error downloading NLTK data: {e}")
        # Return a basic set of common English stopwords as fallback
        return {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 
                'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 
                'him', 'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their', 'am', 
                'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 
                'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'shall'}

# Download NLTK data and get stopwords
stop_words = download_nltk_data()

# --- Load Model and Vectorizer ---
@st.cache_resource
def load_model_and_vectorizer():
    """Loads the saved SVM model and TF-IDF vectorizer."""
    try:
        model = joblib.load('tuned_svm_linear_model.pkl')
        vectorizer = joblib.load('vectorizer.pkl')
        return model, vectorizer
    except FileNotFoundError:
        st.error("Model or vectorizer file not found. Please make sure 'tuned_svm_linear_model.pkl' and 'vectorizer.pkl' are in the same directory.")
        return None, None

model, tfidf_vectorizer = load_model_and_vectorizer()

# --- Text Preprocessing Function ---
def preprocess_text(text):
    """Cleans and preprocesses the input text."""
    # 1. Convert to lowercase
    text = text.lower()
    # 2. Remove punctuation and extra spaces
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    # 3. Tokenization with fallback
    try:
        from nltk.tokenize import word_tokenize
        tokens = word_tokenize(text)
    except Exception as e:
        # Fallback to simple split if NLTK tokenization fails
        st.warning("Using simple tokenization due to NLTK issue")
        tokens = text.split()
    
    # 4. Remove stopwords
    tokens = [token for token in tokens if token not in stop_words]
    # 5. Join tokens back to string
    return ' '.join(tokens)

# --- Custom CSS for Styling ---
st.markdown("""
<style>
/* Main page background */
.main {
    background-color: #FFFFFF; /* White */
}
/* Title color */
h1 {
    color: #800000; /* Maroon */
}
/* Button style */
.stButton>button {
    background-color: #000080; /* Navy Blue */
    color: white;
    border-radius: 5px;
    border: none;
    padding: 10px 20px;
}
.stButton>button:hover {
    background-color: #800000; /* Maroon on hover */
    color: white;
}
/* Text area style */
.stTextArea textarea {
    border: 2px solid #000080; /* Navy Blue border */
    border-radius: 5px;
}
/* Success box for prediction */
.st-success {
    background-color: rgba(0, 0, 128, 0.1); /* Light Navy Blue */
    border: 1px solid #000080; /* Navy Blue */
    border-radius: 5px;
    color: #800000; /* Maroon text */
}
</style>
""", unsafe_allow_html=True)

# --- Streamlit App Interface ---
st.title('ðŸ“° BBC News Article Classifier')

st.markdown("""
Enter the text of a news article below, and the model will predict its category:
**Business, Entertainment, Politics, Sport, or Tech.**
""")
import subprocess
import sys

def install_nltk_data():
    try:
        import nltk
        nltk.download('punkt')
        nltk.download('punkt_tab')
        nltk.download('stopwords')
    except:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "nltk"])
        import nltk
        nltk.download('punkt')
        nltk.download('punkt_tab')
        nltk.download('stopwords')

install_nltk_data()


# User input text area
user_input = st.text_area("Enter article text here:", height=250)

# Prediction button
if st.button('Classify Article'):
    if model and tfidf_vectorizer:
        if user_input.strip():
            # 1. Preprocess the input
            processed_input = preprocess_text(user_input)

            # 2. Vectorize the processed text
            vectorized_input = tfidf_vectorizer.transform([processed_input])

            # 3. Make a prediction
            prediction = model.predict(vectorized_input)

            # 4. Map prediction to category name
            # This mapping must match the LabelEncoder from your notebook
            category_mapping = {
                0: 'Business',
                1: 'Entertainment',
                2: 'Politics',
                3: 'Sport',
                4: 'Tech'
            }
            predicted_category = category_mapping.get(prediction[0], 'Unknown')

            # 5. Display the result
            st.success(f"**Predicted Category:** {predicted_category}")
        else:
            st.warning("Please enter some text to classify.")
    else:
        st.error("Model or vectorizer could not be loaded. Please check your files.")
